\section{Introduction}\label{sec:introduction}

Indian judicial system, like many in the other parts of the world, is based on whats called ``Common Law System'' in which both, written law (called ``statutes'') and prior cases (called ``precedent'') are given equal importance while forming the judgment. Such system brings uniformity of the legal decisions across similar situations. With number of cases increasing day by day, it has become humanly impossible to search relevant past cases for a particular topic. Automatic Precedent Retrieval System (APRS) is the need of the hour. As more and more cases are coming in the digital form, text mining has found immense importance for developing APRS.

Court cases, judgments, legal texts are typically long and unstructured, making it hard to query relevant information from them, unless someone goes through them manually and vigilantly. Looking at the volume of legal text to be processed, it is desirable to have automatic system that detect key concepts, catchphrases in the legal texts.

The aim of this paper is to propose automatic catchphrase detection-prediction system for legal text. It uses training data comprising of pairs of text and respective catchphrases,  the gold standard, prepared manually by legal experts. The proposed system builds probabilistic model based on this training data, which, in turn, can predict the catchphrases in the unseen legal texts.

The contributions made in this paper are as follows:

\begin{enumerate}
	\item A novel method to prepare training data needed for CRF.
	\item Feature engineering for better results with CRF
\end{enumerate}

The paper has been structured as follows: in the following section \ref{sec:taskdef} the catchphrases detection task has been described in details, as definition of the problem. In section \ref{sec:datares}, structure of the training data has been explained. Next, the proposed system is elaborated in Section \ref{sec:sysdescr}. It describes preparation of CRF training data-set and feature engineering adapted for this custom named entity recognition (NER) methodology. Section \ref{sec:conclusions} discusses the findings drawn from this work.


\section{Task Definition}
\label{sec:taskdef}

{\em Catchphrases are short phrases from within the text of the document. Catchphrases can be extracted by selecting certain portions from the text of the document}\cite{fire2017}. The data-set provided consists of legal texts and their respective catchphrases, along with test documents for which the catchphrase needs to be extracted-predicted.

\section{Data-set}
\label{sec:datares} 
Fire-2017 \cite{fire2017} dataset contains following directories:
\begin{enumerate}
	\item Train\_docs : contains 100 case statements, $case\_<i>\_statement.txt$ where $i = 0 \rightarrow 99$. Sample document looks like ``R.P. Sethi, J. 1. Aggrieved by the determination of annual \ldots ultimate result.''.
	\item Train\_catches: contains the gold standard catchwords for each of these 100 case statements, $case\_<i>\_catchwords.txt$ where $i = 0 \rightarrow 99$. Sample document looks like ``Absence, Access, Accident, Account, \ldots Vehicle, Vehicles''.
	\item Test\_docs: contains 300 test case statements, similar to Train\_docs, $case\_<i>\_statement.txt$ where $i = 100 \rightarrow 399$.
\end{enumerate}


\section{System Description}
\label{sec:sysdescr} 

\subsection{Preprocessing}
Each of the training statements were tokenized into a list. Their Parts-of-Speech (POS) tags were generated using python nltk \cite{nltk} library. Another sequence of custom NER tagging was made by referring to token list and given catchphrases. B-LEGAL and I-LEGAL tags were employed for Begin and Intermediate of the catchphrases respectively and  O for other tokens. 
So training data file looked like:
\begin{center}
\begin{tabular}[h]{ l  l  l}
in                 &  IN     &       O \\
the              &  DT     &       O\\
year            &  NN    &       O\\
1987 		&	CD 	&	O\\
and 		&	CC 	&	O\\
that 	     &      IN 	&		O\\
property 	&	NN 	 &    B-LEGAL\\
had 		&	VBD 	&	O\\
extensive& 	JJ 		&	O\\
national 	&	JJ 	      &     B-LEGAL\\
highway 	&     NN 	 &    I-LEGAL\\
frontage 	&	NN 	&	O\\
\end{tabular}
\label{tab:training}
\captionof{table}{Training data with primary features}
\end{center}
There are 3 columns for each token.
\begin{enumerate}
	\item The word itself (e.g. property);
	\item POS associated with the word (e.g. NN);
	\item Custom NER tag  (e.g. B-LEGAL);
\end{enumerate}

\begin{lstlisting}

\end{lstlisting}
Each test statement was also tokenized into a list and its POS tags were generated using nltk \cite{nltk}, so testing data file looked like:

\begin{center}
\begin{tabular}[h]{ l  l }
			appeals    &	NN\\
			the    &	NNS\\
			high   &	DT\\
			court    &	JJ\\
			accepted 	   & NN\\
			the    &	VBD\\
			view    &	DT\\
%			of    &	NN\\
%			the    &	IN\\
%			tribunal    &	DT\\
			\end{tabular}
\label{tab:testing}
\captionof{table}{Testing data with primary feature}
\end{center}
There are 2 columns for each token.
\begin{enumerate}
	\item The word itself (e.g. appeals);
	\item POS associated with the word (e.g. NN);
\end{enumerate}

\subsection{Modeling}
The problem of detecting catchphrases was modeled as customized NER. POS and custom NER tagging performed during pre-processing stage were used to form secondary features. These were used  in building CRF model. CRF++ \cite{crfpp} toolkit was used. Salient secondary features developed were:
\begin{enumerate}
	\item Unigrams:
	\begin{enumerate}
		\item Previous 3 tokens, current token and next 3 tokens
		\item Previous 3 POS tags, current POS tag and next 3 POS tags
	\end{enumerate}
	\item Bigram tokens
\end{enumerate}
CRF model was generated using:

\lstinline|crf_learn template_file train_file model_file|

The generated model file was then used to predict from test data:

\lstinline|crf_test -v1 -m model_file test_files|

With $-v1$ option the highest probability is shown as:
\begin{center}
\begin{tabular}[h]{ l  l  l l}
Rockwell       &	   NNP     &	  B      &	   B/0.992465 \\
International   &	  NNP     &	  I      &	   I/0.979089 \\
Corp.    &	 NNP      &	 I    &	     I/0.954883 \\
\end{tabular}
\label{tab:verbose}
\captionof{table}{Sample results with probabilities}
\end{center}

\subsection{Results}
The CRF++ model was used to predict custom NER tags from the given testing data as:
\begin{center}
\begin{tabular}[h]{ l  l  }
case\_102\_statement & notification:0.990733,tax:0.7341635\\
case\_103\_statement & prevention of corruption:0.9988746666666667\\
case\_104\_statement &  natural justice:0.7491485,appeal:0.708494\\
case\_105\_statement & seniority:0.997623,legislation:0.994512,appointing \\
\end{tabular}
\label{tab:results}
\captionof{table}{Submission file}
\end{center}
There far less catchphrase words compared to total number of words in the documents. Thus, accuracy is not a good metrics to measure the performance of this prediction. ``Precision'' and ``Recall'' values on the testing data-set came out to be as follows:
\begin{center}
\begin{tabular}[h]{ |l |  l|} \hline
{\bf Mean Average Precision}  & {\bf Overall Recall} \\ \hline
0.47923074  & 0.2476876075 \\ \hline
			\end{tabular}
\label{tab:verbose}
\captionof{table}{Results conveyed by FIRE\cite{fire2017}}
\end{center}

\section{Conclusions}
\label{sec:conclusions}
In this paper, a brief overview of Automatic Catchphrases Prediction System was presented to extract catchphrases from legal texts. Accuracy was impacted as some of the training samples had very few catchphrases. Various apporaches/toolkits were tried but it was found that the problem of catchphrase detection needs to be modeled as sequential probabilistic labeling problem rather than a simple linear classification problem. CRF algorithm was chosen with primary features as POS and custom NER tags and numerous secondary features representing the context. 
As a future work, if sufficient gold standard data is available, one can explore more sophisticated techniques such as Long Short Term Memory networks (LSTM), where custom features need not be provided but get generated internally.

\section*{Vitae}

{\bf Yogesh H. Kulkarni} works as Data Science Consultant and Trainer. Profile: https://www.linkedin.com/in/yogeshkulkarni/

\noindent {\bf Rishabh Patil} works as Data Engineer. His profile is at https://www. linkedin.com/in/rishabh-patil-256a25124/.

\noindent {\bf Srinivasan Shridharan} is Data Scientist and entrepreneur. Profile: https://www.linkedin.com/in/srinivasan-shridharan-08a86a6/.
\begin{acks}
  Wish to thank Ankur Parikh, a keen researcher of Deep Learning and NLP, for discussions.
\end{acks}
